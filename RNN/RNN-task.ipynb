{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating names with recurrent neural networks\n",
    "\n",
    "This time you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n",
    "\n",
    "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead;\n",
    "\n",
    "It's dangerous to go alone, take these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.696201Z",
     "start_time": "2018-08-13T20:26:38.104103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import keras_utils\n",
    "import tqdm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "The dataset contains ~8k earthling names from different cultures, all in latin transcript.\n",
    "\n",
    "This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.701832Z",
     "start_time": "2018-08-13T20:26:42.697766Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_token = \" \"  # so that the network knows that we're generating a first token\n",
    "\n",
    "# this is the token for padding,\n",
    "# we will add fake pad token at the end of names \n",
    "# to make them of equal size for further batching\n",
    "pad_token = \"#\"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + name for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.707885Z",
     "start_time": "2018-08-13T20:26:42.703302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 7944\n",
      " Abagael\n",
      " Claresta\n",
      " Glory\n",
      " Liliane\n",
      " Prissie\n",
      " Geeta\n",
      " Giovanne\n",
      " Piggy\n"
     ]
    }
   ],
   "source": [
    "print('number of samples:', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.857411Z",
     "start_time": "2018-08-13T20:26:42.709371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 16\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEICAYAAAC55kg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZhlzCJiZZcwhYGaWMYeAmVnG/j9X9jq2BqwyjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73f809ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "print(\"max length:\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, names)), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "\n",
    "First we need to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.864592Z",
     "start_time": "2018-08-13T20:26:42.858725Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = \n",
    "tokens = list(tokens)\n",
    "n_tokens = len(tokens)\n",
    "print ('n_tokens:', n_tokens)\n",
    "\n",
    "assert 50 < n_tokens < 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers\n",
    "\n",
    "Tensorflow string manipulation is a bit tricky, so we'll work around it. \n",
    "We'll feed our recurrent neural network with ids of characters from our dictionary.\n",
    "\n",
    "To create such dictionary, let's assign `token_to_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.870330Z",
     "start_time": "2018-08-13T20:26:42.866135Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = ### YOUR CODE HERE: create a dictionary of {symbol -> its  index in tokens}\n",
    "\n",
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.875943Z",
     "start_time": "2018-08-13T20:26:42.871834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix(names, max_len=None, pad=token_to_id[pad_token], dtype=np.int32):\n",
    "    \"\"\"Casts a list of names into rnn-digestable padded matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(token_to_id.get, names[i]))\n",
    "        names_ix[i, :len(name_ix)] = name_ix\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.883107Z",
     "start_time": "2018-08-13T20:26:42.877186Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example: cast 4 random names to padded matrices (so that we can easily batch them)\n",
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a recurrent neural network\n",
    "\n",
    "We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
    "<img src=\"./rnn.png\" width=600>\n",
    "\n",
    "Since we're training a language model, there should also be:\n",
    "* An embedding layer that converts character id x_t to a vector.\n",
    "* An output layer that predicts probabilities of next phoneme based on h_t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.039419Z",
     "start_time": "2018-08-13T20:26:42.884581Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember to reset your session if you change your graph!\n",
    "s = keras_utils.reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.044903Z",
     "start_time": "2018-08-13T20:26:44.041084Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import concatenate, Dense, Embedding\n",
    "\n",
    "rnn_num_units = 64  # size of hidden state\n",
    "embedding_size = 16  # for characters\n",
    "\n",
    "# Let's create layers for our recurrent network\n",
    "# Note: we create layers but we don't \"apply\" them yet (this is a \"functional API\" of Keras)\n",
    "# Note: set the correct activation (from keras.activations) to Dense layers!\n",
    "\n",
    "# an embedding layer that converts character ids into embeddings\n",
    "embed_x = Embedding(n_tokens, embedding_size)\n",
    "\n",
    "# a dense layer that maps input and previous state to new hidden state, [x_t,h_t]->h_t+1\n",
    "get_h_next = ### YOUR CODE HERE\n",
    "\n",
    "# a dense layer that maps current hidden state to probabilities of characters [h_t+1]->P(x_t+1|h_t+1)\n",
    "get_probas = ### YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate names character by character starting with `start_token`:\n",
    "\n",
    "<img src=\"./char-nn.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.053212Z",
     "start_time": "2018-08-13T20:26:44.048389Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_one_step(x_t, h_t):\n",
    "    \"\"\"\n",
    "    Recurrent neural network step that produces \n",
    "    probabilities for next token x_t+1 and next state h_t+1\n",
    "    given current input x_t and previous state h_t.\n",
    "    We'll call this method repeatedly to produce the whole sequence.\n",
    "    \n",
    "    You're supposed to \"apply\" above layers to produce new tensors.\n",
    "    Follow inline instructions to complete the function.\n",
    "    \"\"\"\n",
    "    # convert character id into embedding\n",
    "    x_t_emb = embed_x(tf.reshape(x_t, [-1, 1]))[:, 0]\n",
    "    \n",
    "    # concatenate x_t embedding and previous h_t state\n",
    "    x_and_h = ### YOUR CODE HERE\n",
    "    \n",
    "    # compute next state given x_and_h\n",
    "    h_next = ### YOUR CODE HERE\n",
    "    \n",
    "    # get probabilities for language model P(x_next|h_next)\n",
    "    output_probas = ### YOUR CODE HERE\n",
    "    \n",
    "    return output_probas, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: loop\n",
    "\n",
    "Once `rnn_one_step` is ready, let's apply it in a loop over name characters to get predictions.\n",
    "\n",
    "Let's assume that all names are at most length-16 for now, so we can simply iterate over them in a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.342948Z",
     "start_time": "2018-08-13T20:26:44.056136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = tf.placeholder(tf.int32, (None, MAX_LENGTH))  # batch of token ids\n",
    "batch_size = tf.shape(input_sequence)[0]\n",
    "\n",
    "predicted_probas = []\n",
    "h_prev = tf.zeros([batch_size, rnn_num_units])  # initial hidden state\n",
    "\n",
    "for t in range(MAX_LENGTH):\n",
    "    x_t = input_sequence[:, t]  # column t\n",
    "    probas_next, h_next = rnn_one_step(x_t, h_prev)\n",
    "    \n",
    "    h_prev = h_next\n",
    "    predicted_probas.append(probas_next)\n",
    "    \n",
    "# combine predicted_probas into [batch, time, n_tokens] tensor\n",
    "predicted_probas = tf.transpose(tf.stack(predicted_probas), [1, 0, 2])\n",
    "\n",
    "# next to last token prediction is not needed\n",
    "predicted_probas = predicted_probas[:, :-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: loss and gradients\n",
    "\n",
    "Let's gather a matrix of predictions for $P(x_{next}|h)$ and the corresponding correct answers.\n",
    "\n",
    "We will flatten our matrices to shape [None, n_tokens] to make it easier.\n",
    "\n",
    "Our network can then be trained by minimizing crossentropy between predicted probabilities and those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.354310Z",
     "start_time": "2018-08-13T20:26:44.344648Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten predictions to [batch*time, n_tokens]\n",
    "predictions_matrix = tf.reshape(predicted_probas, [-1, n_tokens])\n",
    "\n",
    "# flatten answers (next tokens) and one-hot encode them\n",
    "answers_matrix = tf.one_hot(tf.reshape(input_sequence[:, 1:], [-1]), n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it's a good idea to ignore gradients of loss for padding token predictions.\n",
    "\n",
    "Because we don't care about further prediction after the pad_token is predicted for the first time, so it doesn't make sense to punish our network after the pad_token is predicted.\n",
    "\n",
    "For simplicity you can ignore this comment, it's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:45.076642Z",
     "start_time": "2018-08-13T20:26:44.355594Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the loss as categorical cross-entropy (e.g. from keras.losses).\n",
    "# Mind that predictions are probabilities and NOT logits!\n",
    "# Remember to apply tf.reduce_mean to get a scalar loss!\n",
    "loss = ### YOUR CODE HERE\n",
    "\n",
    "optimize = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:55.322187Z",
     "start_time": "2018-08-13T20:26:45.078296Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 32\n",
    "history = []\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = to_matrix(sample(names, batch_size), max_len=MAX_LENGTH)\n",
    "    loss_i, _ = s.run([loss, optimize], {input_sequence: batch})\n",
    "    \n",
    "    history.append(loss_i)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: sampling\n",
    "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:55.341196Z",
     "start_time": "2018-08-13T20:26:55.323787Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_t = tf.placeholder(tf.int32, (1,))\n",
    "h_t = tf.Variable(np.zeros([1, rnn_num_units], np.float32))  # we will update hidden state in this variable\n",
    "\n",
    "# For sampling we need to define `rnn_one_step` tensors only once in our graph.\n",
    "# We reuse all parameters thanks to functional API usage.\n",
    "# Then we can feed appropriate tensor values using feed_dict in a loop.\n",
    "# Note how different it is from training stage, where we had to unroll the whole sequence for backprop.\n",
    "next_probs, next_h = rnn_one_step(x_t, h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:55.346422Z",
     "start_time": "2018-08-13T20:26:55.342659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(seed_phrase=start_token, max_length=MAX_LENGTH):\n",
    "    '''\n",
    "    This function generates text given a `seed_phrase` as a seed.\n",
    "    Remember to include start_token in seed phrase!\n",
    "    Parameter `max_length` is used to set the number of characters in prediction.\n",
    "    '''\n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    s.run(tf.assign(h_t, h_t.initial_value))\n",
    "    \n",
    "    # feed the seed phrase, if any\n",
    "    for ix in x_sequence[:-1]:\n",
    "         s.run(tf.assign(h_t, next_h), {x_t: [ix]})\n",
    "    \n",
    "    # start generating\n",
    "    for _ in range(max_length-len(seed_phrase)):\n",
    "        x_probs,_ = s.run([next_probs, tf.assign(h_t, next_h)], {x_t: [x_sequence[-1]]})\n",
    "        x_sequence.append(np.random.choice(n_tokens, p=x_probs[0]))\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence if tokens[ix] != pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:58.458115Z",
     "start_time": "2018-08-13T20:26:55.347900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without prefix\n",
    "for _ in range(10):\n",
    "    print(generate_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:01.986726Z",
     "start_time": "2018-08-13T20:26:58.459810Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with prefix conditioning\n",
    "for _ in range(10):\n",
    "    print(generate_sample(' Trump'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:40:02.004926Z",
     "start_time": "2018-08-13T20:40:02.000821Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"### YOUR TOKEN HERE ###\"\n",
    "COURSERA_EMAIL = \"### YOUR EMAIL HERE ###\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:40:18.923357Z",
     "start_time": "2018-08-13T20:40:03.549343Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from submit import submit_char_rnn\n",
    "samples = [generate_sample(' Al') for i in tqdm_utils.tqdm_notebook_failsafe(range(25))]\n",
    "submission = (history, samples)\n",
    "submit_char_rnn(submission, COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it out!\n",
    "\n",
    "__Disclaimer:__ This part of assignment is entirely optional. You won't receive bonus points for it. However, it's a fun thing to do. Please share your results on course forums.\n",
    "\n",
    "You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n",
    "\n",
    "* Novels/poems/songs of your favorite author\n",
    "* News titles/clickbait titles\n",
    "* Source code of Linux or Tensorflow\n",
    "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
    "* Melody in notes/chords format\n",
    "* IKEA catalog titles\n",
    "* Pokemon names\n",
    "* Cards from Magic, the Gathering / Hearthstone\n",
    "\n",
    "If you're willing to give it a try, here's what you wanna look at:\n",
    "* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n",
    "* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n",
    "* Make sure MAX_LENGTH is adjusted for longer datasets. There's also a bonus section about dynamic RNNs at the bottom.\n",
    "* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n",
    "* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n",
    "\n",
    "__Good hunting!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bonus level: dynamic RNNs\n",
    "\n",
    "Apart from Keras, there's also a friendly TensorFlow API for recurrent neural nets. It's based around the symbolic loop function (aka [tf.scan](https://www.tensorflow.org/api_docs/python/tf/scan)).\n",
    "\n",
    "RNN loop that we implemented for training can be replaced with single TensorFlow instruction: [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn).\n",
    "This interface allows for dynamic sequence length and comes with some pre-implemented architectures.\n",
    "\n",
    "Take a look at [tf.nn.rnn_cell.BasicRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:12.975354Z",
     "start_time": "2018-08-13T20:27:12.737529Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomRNN(tf.nn.rnn_cell.BasicRNNCell):\n",
    "    def call(self, input, state):\n",
    "        # from docs:\n",
    "        # Returns:\n",
    "        # Output: A 2-D tensor with shape [batch_size, self.output_size].\n",
    "        # New state: Either a single 2-D tensor, or a tuple of tensors matching the arity and shapes of state.\n",
    "        return rnn_one_step(input[:, 0], state)\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return n_tokens\n",
    "    \n",
    "cell = CustomRNN(rnn_num_units)\n",
    "\n",
    "input_sequence = tf.placeholder(tf.int32, (None, None))\n",
    "    \n",
    "predicted_probas, last_state = tf.nn.dynamic_rnn(cell, input_sequence[:, :, None], dtype=tf.float32)\n",
    "\n",
    "print('LSTM outputs for each step [batch,time,n_tokens]:')\n",
    "print(predicted_probas.eval({input_sequence: to_matrix(names[:10], max_len=50)}).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we never used MAX_LENGTH in the code above: TF will iterate over however many time-steps you gave it.\n",
    "\n",
    "You can also use any pre-implemented RNN cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:12.981697Z",
     "start_time": "2018-08-13T20:27:12.977590Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for obj in dir(tf.nn.rnn_cell) + dir(tf.contrib.rnn):\n",
    "    if obj.endswith('Cell'):\n",
    "        print(obj, end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:13.168207Z",
     "start_time": "2018-08-13T20:27:12.986884Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = tf.placeholder(tf.int32, (None, None))\n",
    "\n",
    "inputs_embedded = embed_x(input_sequence)\n",
    "\n",
    "# standard cell returns hidden state as output!\n",
    "cell = tf.nn.rnn_cell.LSTMCell(rnn_num_units)\n",
    "\n",
    "state_sequence, last_state = tf.nn.dynamic_rnn(cell, inputs_embedded, dtype=tf.float32)\n",
    "\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "print('LSTM hidden state for each step [batch,time,rnn_num_units]:')\n",
    "print(state_sequence.eval({input_sequence: to_matrix(names[:10], max_len=50)}).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
